\section{Methodology}
In this section several points regarding the methodology used in the work will be presented. The general approach taken to modeling, as well as the models themselves will be explained. The metrics used to evaluate the goodness of fit of the different models will also be presented. 

\subsection{Overview of modeling approach}
In order to understand why the models that will be presented below have been chosen and why the evaluation metrics have been chosen, it is worth first taking a look at the requirements of the modeling task and the general criteria that determine what a good result is.

\subsubsection{Modeling task characteristics}
The general technical characteristics of the task, from the objectives and the time series characteristics already explored are the following:
\begin{itemize}
    \item \textbf{Modeling horizon}: The horizon to be modeled is of between 1 and 10 years, or between 8.760 and 87.600 hourly timesteps. 
    \item \textbf{Available data}: The data available for the training and testing tasks consists of slightly below 10 years of data, or slightly below 87.600 observations. This data has been consistently recorded at a 1 hour timestep interval.
    \item \textbf{Seasonality}: There has been found to be high seasonality, both at the daily and yearly frequencies. 
    \item \textbf{Exogenous variables}: A certain predictive power of some series with respect to the others has been found. Thus the capacity of incorporating the use of exogenous variables or multivariate modeling aproaches are specially interesting. 
    \item \textbf{Extreme values}: There is a special interest in having accurate modeling of the extreme values, specially in the wind capacity factor.
    \item \textbf{Pointwise forecast}: The forecasts produced by the model can be pointwise predictions, there is no need to produce probabilistic forecasts for each timestep.
\end{itemize}

\subsubsection{Modeling criteria}
Starting with the objectives already outlined in the section called \nameref{sec:objective-and-scope}, here these objectives will be broken down into more specific criteria that the best model must aim to meet in the best possible way. The specific characteristics the model must meet are the following:
\begin{itemize}
    \item \textbf{Seasonality}: The chosen models must be able of incorporating seasonality into its forecast. 
    \item \textbf{Multiple variables}: The chosen models should be able to incorporate multiple variables, be it through a multivariate modeling, through historical exogenous variables and/or through future exogenous variables. Through the modeling task it will be made apparent wether incorporating the information of the other series helps in modeling a given series.
    \item \textbf{Complex relationships}: The temporal relationships of a series with itself and the relationships of a series with the other variables are probably more complex than linear relationships. That is why a model capable of incorporating complex non linear temporal and intervariate relationships will probably have an advantage.
    \item \textbf{Multiperiod modeling}: As it has already been mentioned, the modeling period will encompass between 8.760 and 87.600 timesteps. The model must be able to create such forecasts. But going further, if the prediction is created by autoregressively estimating the next step, there is a compounding in errors over time \cite{educing_error_propagation_long}. By estimating multiple timesteps in one prediction pass, the number of autoregressive steps can be reduced and with it the compounding of errors can be reduced too. Therefore, models with bigger single-step prediction window should have an advantage.
    \item \textbf{Long and short term temporal relationships}: The model must be able to incorporate both long term and short term temporal relationships.
    \item \textbf{Robustness}: The model must be resistant to overfitting, given the parameter size of some of the models and the number of available training datapoints. 
    \item \textbf{Extreme value accuracy}: The model must be specially sensitive to extreme values, be it through the flexibility of incorporating a custom loss function or through its intrinsic characteristics.
    \item \textbf{Computational efficiency}: Given the long modeling horizon, the model must be efficient both in prediction time and in memory used when training and when incorporating historical data in prediction.
\end{itemize}
\subsection{Models}
Now that the criteria used to select potential models has been outlined, the different models that will be evaluated can be presented. These models have been divided by model family or type. Here below, the different model types and each of the models and their characteristics and main working principles are explained. 
\subsubsection{Statistical models}
These models are based on classical statistical methods to analyze and predict temporal series. They use techniques like regression analysis, autocorrelation and spectral analysis to model and forecast temporal variables. The ones that will be used here are:
\begin{itemize}
    \item \textbf{Linear Regression}: This is the most simple model and it will be used as a naive benchmark. It models a given capacity factor value for one of the series as a linear combination of several exogenous variables. In this case, those variables are the values of fourier sine and cosine terms, some lagged values of the given time series and some lagged values of other time series, which in this case will be the other capacity factors. The main parameters of the model are thus:
    \begin{itemize}
        \item Seasonal frequencies: That is the frequencies for which the fourier terms will be calculated. It can be daily, yearly, both, etc.
        \item Fourier harmonics: For each of the seasonalities, several harmonics of fourier terms can be calculated, creating more nuanced patterns the more harmonics are added.
        \item Autoregressive order: That is, what lags of the modeled series to incorporate.
        \item Exogenous lags: That is, what lags of the other two series to incorporate. 
    \end{itemize}
    The mathematical formulation for the model thus is:
    \begin{multline}
        Y_t=\beta_0 + \sum_{\tau}\sum_{k}\left[\gamma_{\tau,k}\sin\left(\frac{2\pi k t}{\tau}\right) + \delta_{\tau,k}\cos\left(\frac{2\pi k t}{\tau}\right)\right] +\\+ \sum_l \phi_l Y_{t-l} + \sum_v\sum_p\theta_{v,p}X_{v,t-p} + \epsilon_t
    \end{multline}
    where $\tau$ is each of the seasonality periods, $k$ is each of the fourier harmonics, $l$ is each of the self lags, $v$ is each of the other two capacity factors and $p$ is each of the lags used for the two other capacity factors.
    
    This model opperates on several assumptions which are worth mentioning. The first is that all relationships -- those with the seasonal terms, with lagged values, with the other capacity factors, etc. -- are assumed to be linear. The error term $\epsilon_t$ is assumed to be independent and identically distributed (i.i.d.) with constant variance (homocedastic) and gaussian, although there is no evidence that this is true. 
    \item \textbf{ARIMAX}: The AutoRegressive Integrated Moving Average with eXogenous variables model is an extension to the previous model. It incorporates both autoregressive and moving average components to the endogenous time series and allows for time  integration, which extends the purely autoregressive view in the previous model, as well as some exogneous variables. It provides a more complex representation of the relationship of the time series with its lagged values than the linear regression model presented above. To the previously presented parameter, these new ones are added:
    \item \begin{itemize}
        \item Integration order: This represents the number of times the endogenous time series is differentiated. 
        \item Moving average order: That is, what moving average error terms are incorporated.
    \end{itemize}  
    The new model formulation is:
    \begin{multline}
        Y_t=-\left(\Delta^d Y_t - Y_t\right) + \beta_0 + \sum_{\tau}\sum_{k}\left[\gamma_{\tau,k}\sin\left(\frac{2\pi k t}{\tau}\right) + \delta_{\tau,k}\cos\left(\frac{2\pi k t}{\tau}\right)\right] +\\+ \sum_l \phi_l \Delta^d Y_{t-l} + \sum_q \theta_q \epsilon_{t-q} + \sum_v\sum_p\theta_{v,p}X_{v,t-p} + \epsilon_t
    \end{multline}
    where $d$ is the order of differentiation with $\Delta Y_t = Y_t - Y_{t-1}$ and $q$ is lags of the error term added for the moving average.
\end{itemize}
\subsubsection{Machine Learning models}
\subsubsection{Neural Network models}
\subsubsection{Transformer models}

\subsection{Model training and validation (Maybe?)}

\subsection{Evaluation metrics}
In order to understand why the following metrics have been chosen for evaluation it is important to keep in mind the overarching goal of the project. The goal of the mentioned models is not to provide an accurate short term forecast, but to provide a model that porperly simulates the long term dynamics of the three time series. That is, models that are able of producing long term scenarios of solar and wind generation with univariate and multivariate dynamics equivalent to the real world ones, in order to optimize and test several grid planning possiblities on these scenarios. 

In order to best achieve this goal, the best performing model will have to be evaluated on different metrics, each corresponding to different attributes needed for correct whole picture modeling. These are the different attributes that will be tested and the metrics that will be used to test them. 

\subsubsection{Marginal distributions}
The first metric that will be evaluated will be the similarity of the univariate series with the real world data. Note that this similarity is not a pointwise similarity. In many relevant papers, the accuracy of a series predicition is calculated through metrics like the RMSE or the MAPE. However, these metrics require that the prediction try to get right the value at each timestep. If there is a delay of one timestep, or a "cloudy week" comes a week earlier or later the RMSE will give a very poor score even though the general behaviour is the same. That is why the approach taken here will be slightly different. The metrics used will be the following:
\begin{itemize}
    \item \textbf{Cramér von Mises criterion}: The Cramér von Mises criterion intriduced by \cite{cramer_28} measures the distance between two CDF. It is calculated like:
    \begin{equation}
        \omega^2=\int_{-\infty}^{\infty}\left[F_m\left(x\right)-F_e\left(x\right)\right]^2dF_e\left(x\right)
    \end{equation}
    Where $F_m\left(x\right)$ is the modelled empirical CDF and $F_e\left(x\right)$ is the observed empirical CDF.
    \item \textbf{Kullback-Leibler divergence}: The KL divergence or relative entropy introduced in \cite{kullback_leibler_1951} is a statistical distance that measures how different a probability distribution is from another reference one. It is calculated like
    \begin{equation}
        D_{KL}\left(P||Q\right)=\sum_x P\left(x\right)\log{\left(\frac{P\left(x\right)}{Q\left(x\right)}\right)}
    \end{equation}
    where $P$ is the modelled probability distribution and Q is the reference probability distribution.
    \item \textbf{ACF distance}: Even though the marginal distributions are by themselves a fundamental part of the closeness of the univariate series to reality, they are not the only one. Another key aspect of the series is its termporal self dependency, studied through the ACF of the correlation coefficient and the new correlation coefficient in the section called \nameref{sec:lagged-relationships}. The difference between the ACF of the $\xi$ metric of the empirical data and the model will be measured through a weighted root mean squared error, calculated as:
    \begin{equation}
        \text{ACFD}=\sqrt{\frac{\sum_{k=0}^N w_k\left[ACF_m\left(k\right)-ACF_e\left(k\right)\right]^2}{\sum_{k=0}^N w_k}}
    \end{equation}
    where $N$ is the number of periods for which the metric is calculated -- 72 in this case, $ACF_m$ is the model's $\xi$ ACF, $ACF_e$ is the empirical $\xi$ ACF and $w$ is the weighting factor, determined to be $w_k=ACF_e\left(k\right)$. This has been chosen so a higher weight is given to those lags with a higher empirical relevance. 
    \item \textbf{Seasonal metrics}: The metrics above will be calculated on the overall sample but also on subsamples divided by yearly season and daily period, to ensure the fit not only on a global scale but also on specific periods. 
\end{itemize}
\subsubsection{Joint distributions}
Now that it is clear how the marginal distributions will be assessed, it is time to look at how the dependence structure between the three series will be tested. There are several metrics that will be used for that purpose:
\begin{itemize}
    \item \textbf{Copula correlation matrix distance}: In the section regarding \nameref{sec:multivariate-analysis}, the correlation matrix of a gaussian copula representing the dependence structure between the series was calculated. The same matrix will be calculated for samples of each of the models. Then, the distance (CCMD) between both matrices will be calculated. This distance is calculated as:
    \begin{equation}
        CCMD=\sqrt{\sum_i\sum_j\left(a_{ij}-b_{ij}\right)^2}
    \end{equation}
    where $A=\{a_{ij}\}$ is the correlation matrix of the copula fitted to the modelled data and $B=\{b_{ij}\}$ is the correlation matrix shown in \autoref{table:copula-correlation-matrix}.
    \item \textbf{CCF distance}: The cross correlation function distance is analogous to the autocorrelation function distance explained above, but for the $\xi$ cross correlation function instead of ACF.
    \item \textbf{Seasonal metrics}: Similarly to the univariate case, the metrics above will be calculated on the whole period but also with seasonal differences and daily period differences.
\end{itemize}

\subsubsection{Extreme value analysis}
A significantly important aspect of the models is their need to be particularly accurate on the extreme values. Periods of extreme wind or solar generation are particularly interesting to the grid, due to their displacement of other energy sources or their need of them. That is why the accuracy of the models on extreme values will be assessed through these metrics:
\begin{itemize}
    % \item \textbf{Anderson-Darling statistic}:
    \item \textbf{Conditional Value at Risk}: The CVaR, usually used in financial risk management, measures what a certain value on average will be given that a certain threshold has been exceeded. It is calculated as 
    \begin{equation}
        \text{CVaR}_{\alpha}(x) = \mathbb{E} \left[X\mid X>\text{VaR}_{\alpha}\right]
    \end{equation} 
    with $\text{VaR}_{\alpha}$ being the Value at Risk for a given certainty. That is, the maximum value not exceeded with a probability $\alpha$. It is characterized as 
    \begin{equation}
        \text{VaR}_{\alpha}=\text{inf}\{x|F_X(x)\geq\alpha\}
    \end{equation}
    The assessment metric will be the difference between the CVaR calculated as 
    \begin{equation}
        \text{CVRD}=\frac{\text{CVaR}^m_{95\%}}{\text{CVaR}^e_{95\%}}-1
    \end{equation}
    where $\text{CVaR}^m_{95\%}$ is the CVaR at a 95\% level for the model and $\text{CVaR}^e_{95\%}$ is the empirical CVaR at a 95\% level.
    \item \textbf{Tail dependence}: The tail dependence coefficient is a measure of the comovements of the tails of their distributions. It can be lower tail dependence or upper tail dependence, with the lower tail dependence calculated as 
    \begin{equation}
        \lambda_l=\lim_{q \to 0} P\left(X_m \leq F^{-1}_m(q) \mid X_e \leq F^{-1}_e(q)\right)  
    \end{equation}
    and the upper tail dependence coefficient being calculated as 
    \begin{equation}
        \lambda_u=\lim_{q \to 1} P\left(X_m > F^{-1}_m(q) \mid X_e > F^{-1}_e(q)\right)  
    \end{equation}
    with $F^{-1}_m$ being the inverse CDF for the model and $F^{-1}_e$ being the empirical inverse CDF.

    There will be two comparisons. The comparison of the tail dependence between each pair of modeled variables with that of the corresponding pair of empirical variables -- e.g. tail dependence between modeled solar PV and modeled wind and tail dependence between empirical solar PV and empirical wind -- and also the tail dependence between each modeled variable with its corresponding empirical varaiable -- e.g. tail dependence between modeled solar PV with empirical solar PV.  
    \item \textbf{Return level}: The GEV function is a function often used to model the maxima of sequences of random variables. Its CDF is
    \begin{equation}
        P\left(GEV\left(\mu,\sigma,\xi\right)\leq x\right)=e^{-t\left(x\right)}
    \end{equation}
    with
    \begin{equation}
        t\left(x\right)\equiv 
        \begin{cases} 
        \left[ 1 + \xi \left( \frac{x - \mu}{\sigma} \right) \right]^{- \frac{1}{\xi}}, & \text{if } \xi \neq 0, \\
        e^{\left( - \frac{x - \mu}{\sigma} \right)}, & \text{if } \xi = 0.
        \end{cases}
    \end{equation}
    As it can be seen from the formulation above the distribution of maximum values depends on three variables, with $\xi$ governing the tail behaviour. Once the GEV distribution is fitted to the data, the return level can be estimated. The return level is the value expected to be exceeded on average once in a given period. If the 100 year return level of wind capacity factor is 0.95, that means that on average there will be one instance every 100 years where the capacity factor exceeds 0.95. The return level $z_T$ is calcualted as 
    \begin{equation}
        z_T=\mu+\frac{\sigma}{\xi}\left[\left(-\text{log}\left(1-\frac{1}{T}\right)\right)^{-\xi}-1\right]
    \end{equation}
    Thus, the weekly maxima will be obtained with the data, with which a GEV distribution will be fitted and a 10 year return level will be calculated. The assessment metric will be the percentage difference in return level calculated as 
    \begin{equation}
        \text{RLD}=\frac{z^m_{10}}{z^e_{10}}-1
    \end{equation}
    This will be calculated for both the maxima and the minima of the distributions. 
\end{itemize}

The extreme value assessment is particularly important for wind generation, rather than for solar generation as it is much more consistent. Therefore, these values will only be calculated for the wind series. 